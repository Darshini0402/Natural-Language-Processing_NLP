{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize Python NLTK (Natural Language Tool Kit) Platform and do the following.\n",
    "Install relevant Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.corpus import brown\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Explore Brown Corpus and find the size, tokens, categories,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of words in Brown Corpus:  1161192\n",
      "Tokens in Brown Corpus:  ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
      "Categories in Brown Corpus:  ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of words in Brown Corpus: \",len(brown.words()))\n",
    "print(\"Tokens in Brown Corpus: \",brown.words())\n",
    "print(\"Categories in Brown Corpus: \",brown.categories())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Find the size of word tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of words in Brown Corpus:  1161192\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of words in Brown Corpus: \",len(brown.words()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Find the size of word types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "AT \t-\t 97959\n",
      "NP-TL \t-\t 4019\n",
      "NN-TL \t-\t 13372\n",
      "JJ-TL \t-\t 4107\n",
      "VBD \t-\t 26167\n",
      "NR \t-\t 1566\n",
      "NN \t-\t 152470\n",
      "IN \t-\t 120557\n",
      "NP$ \t-\t 2565\n",
      "JJ \t-\t 64028\n",
      "`` \t-\t 8837\n",
      "'' \t-\t 8789\n",
      "CS \t-\t 22143\n",
      "DTI \t-\t 2921\n",
      "NNS \t-\t 55110\n",
      ". \t-\t 60638\n",
      "RBR \t-\t 1182\n",
      ", \t-\t 58156\n",
      "WDT \t-\t 5539\n",
      "HVD \t-\t 4895\n",
      "VBZ \t-\t 7373\n",
      "CC \t-\t 37718\n",
      "IN-TL \t-\t 1477\n",
      "BEDZ \t-\t 9806\n",
      "VBN \t-\t 29186\n",
      "NP \t-\t 34476\n",
      "BEN \t-\t 2470\n",
      "TO \t-\t 14918\n",
      "VB \t-\t 33693\n",
      "RB \t-\t 36464\n",
      "DT \t-\t 8957\n",
      "PPS \t-\t 18253\n",
      "DOD \t-\t 1047\n",
      "AP \t-\t 9522\n",
      "BER \t-\t 4379\n",
      "HV \t-\t 3928\n",
      "DTS \t-\t 2435\n",
      "VBG \t-\t 17893\n",
      "PPO \t-\t 11181\n",
      "QL \t-\t 8735\n",
      "JJT \t-\t 1005\n",
      "ABX \t-\t 730\n",
      "NN-HL \t-\t 1471\n",
      "VBN-HL \t-\t 137\n",
      "WRB \t-\t 4509\n",
      "CD \t-\t 13510\n",
      "MD \t-\t 12431\n",
      "BE \t-\t 6360\n",
      "JJR \t-\t 1958\n",
      "VBG-TL \t-\t 133\n",
      "BEZ \t-\t 10066\n",
      "NN$-TL \t-\t 361\n",
      "HVZ \t-\t 2433\n",
      "ABN \t-\t 3010\n",
      "PN \t-\t 2573\n",
      "PPSS \t-\t 13802\n",
      "PP$ \t-\t 16872\n",
      "DO \t-\t 1353\n",
      "NN$ \t-\t 1480\n",
      "NNS-HL \t-\t 609\n",
      "WPS \t-\t 3924\n",
      "* \t-\t 4603\n",
      "EX \t-\t 2164\n",
      "VB-HL \t-\t 125\n",
      ": \t-\t 1558\n",
      "( \t-\t 2264\n",
      ") \t-\t 2273\n",
      "NNS-TL \t-\t 2226\n",
      "NPS \t-\t 1275\n",
      "JJS \t-\t 359\n",
      "RP \t-\t 6009\n",
      "-- \t-\t 3405\n",
      "BED \t-\t 3282\n",
      "OD \t-\t 1935\n",
      "BEG \t-\t 686\n",
      "AT-HL \t-\t 332\n",
      "VBG-HL \t-\t 146\n",
      "AT-TL \t-\t 746\n",
      "PPL \t-\t 1233\n",
      "DOZ \t-\t 467\n",
      "NP-HL \t-\t 517\n",
      "NR$ \t-\t 66\n",
      "DOD* \t-\t 402\n",
      "BEDZ* \t-\t 154\n",
      ",-HL \t-\t 171\n",
      "CC-TL \t-\t 307\n",
      "MD* \t-\t 866\n",
      "NNS$ \t-\t 257\n",
      "PPSS+BER \t-\t 278\n",
      "' \t-\t 317\n",
      "PPSS+BEM \t-\t 270\n",
      "CD-TL \t-\t 898\n",
      "RBT \t-\t 101\n",
      "(-HL \t-\t 162\n",
      ")-HL \t-\t 184\n",
      "MD-HL \t-\t 27\n",
      "VBZ-HL \t-\t 72\n",
      "IN-HL \t-\t 508\n",
      "JJ-HL \t-\t 396\n",
      "PPLS \t-\t 345\n",
      "CD-HL \t-\t 444\n",
      "WPO \t-\t 280\n",
      "JJS-TL \t-\t 20\n",
      "ABL \t-\t 357\n",
      "BER-HL \t-\t 11\n",
      "PPS+HVZ \t-\t 43\n",
      "VBD-HL \t-\t 8\n",
      "RP-HL \t-\t 14\n",
      "MD*-HL \t-\t 1\n",
      "AP-HL \t-\t 40\n",
      "CS-HL \t-\t 25\n",
      "DT$ \t-\t 5\n",
      "HVN \t-\t 237\n",
      "FW-IN \t-\t 84\n",
      "FW-DT \t-\t 2\n",
      "VBN-TL \t-\t 591\n",
      "NR-TL \t-\t 309\n",
      "NNS$-TL \t-\t 74\n",
      "FW-NN \t-\t 288\n",
      "HVG \t-\t 281\n",
      "DTX \t-\t 104\n",
      "OD-TL \t-\t 201\n",
      "BEM \t-\t 226\n",
      "RB-HL \t-\t 49\n",
      "PPSS+MD \t-\t 484\n",
      "NPS-HL \t-\t 8\n",
      "NPS$ \t-\t 38\n",
      "WP$ \t-\t 252\n",
      "NN-TL-HL \t-\t 129\n",
      "CC-HL \t-\t 119\n",
      "PPS+BEZ \t-\t 430\n",
      "AP-TL \t-\t 18\n",
      "UH-TL \t-\t 15\n",
      "BEZ-HL \t-\t 30\n",
      "TO-HL \t-\t 55\n",
      "DO* \t-\t 485\n",
      "VBN-TL-HL \t-\t 6\n",
      "NNS-TL-HL \t-\t 14\n",
      "DT-HL \t-\t 6\n",
      "BE-HL \t-\t 13\n",
      "DOZ* \t-\t 89\n",
      "QLP \t-\t 261\n",
      "JJR-HL \t-\t 17\n",
      "PPSS+HVD \t-\t 83\n",
      "FW-IN+NN \t-\t 5\n",
      "PP$$ \t-\t 164\n",
      "JJT-HL \t-\t 6\n",
      "NP-TL-HL \t-\t 7\n",
      "NPS-TL \t-\t 67\n",
      "MD+HV \t-\t 7\n",
      "NP$-TL \t-\t 141\n",
      "OD-HL \t-\t 8\n",
      "JJR-TL \t-\t 15\n",
      "VBD-TL \t-\t 6\n",
      "DT+BEZ \t-\t 179\n",
      "EX+BEZ \t-\t 105\n",
      "PPSS+HV \t-\t 241\n",
      ":-HL \t-\t 138\n",
      "PPS+MD \t-\t 144\n",
      "UH \t-\t 608\n",
      "FW-CC \t-\t 27\n",
      "FW-NNS \t-\t 83\n",
      "BEDZ-HL \t-\t 1\n",
      "NN$-HL \t-\t 20\n",
      ".-HL \t-\t 598\n",
      "HVD* \t-\t 99\n",
      "BEZ* \t-\t 117\n",
      "AP$ \t-\t 9\n",
      "NP+BEZ \t-\t 25\n",
      "FW-AT-TL \t-\t 44\n",
      "VB-TL \t-\t 96\n",
      "RB-TL \t-\t 40\n",
      "MD-TL \t-\t 8\n",
      "PN+HVZ \t-\t 3\n",
      "FW-JJ-TL \t-\t 74\n",
      "FW-NN-TL \t-\t 170\n",
      "ABN-HL \t-\t 4\n",
      "PPS+BEZ-HL \t-\t 1\n",
      "NR-HL \t-\t 10\n",
      "HVD-HL \t-\t 1\n",
      "RB$ \t-\t 9\n",
      "FW-AT-HL \t-\t 1\n",
      "DO-HL \t-\t 4\n",
      "PP$-TL \t-\t 35\n",
      "FW-IN-TL \t-\t 40\n",
      "WPS+BEZ \t-\t 21\n",
      "*-HL \t-\t 8\n",
      "DTI-HL \t-\t 6\n",
      "PN-HL \t-\t 2\n",
      "CD$ \t-\t 5\n",
      "BER* \t-\t 47\n",
      "NNS$-HL \t-\t 4\n",
      "PN$ \t-\t 89\n",
      "BER-TL \t-\t 6\n",
      "TO-TL \t-\t 10\n",
      "FW-JJ \t-\t 53\n",
      "BED* \t-\t 22\n",
      "RB+BEZ \t-\t 11\n",
      "VB+PPO \t-\t 71\n",
      "PPSS-HL \t-\t 25\n",
      "HVZ* \t-\t 22\n",
      "FW-IN+NN-TL \t-\t 2\n",
      "FW-IN+AT-TL \t-\t 18\n",
      "NN-NC \t-\t 118\n",
      "JJ-NC \t-\t 41\n",
      "NR$-TL \t-\t 11\n",
      "FW-PP$-NC \t-\t 1\n",
      "FW-VB \t-\t 26\n",
      "FW-VB-NC \t-\t 3\n",
      "JJR-NC \t-\t 5\n",
      "NPS$-TL \t-\t 3\n",
      "QL-TL \t-\t 6\n",
      "FW-AT \t-\t 24\n",
      "FW-* \t-\t 6\n",
      "FW-CD \t-\t 7\n",
      "WQL \t-\t 176\n",
      "FW-WDT \t-\t 16\n",
      "WDT+BEZ \t-\t 47\n",
      "PPO-HL \t-\t 5\n",
      "JJ-TL-HL \t-\t 26\n",
      "NPS$-HL \t-\t 1\n",
      "UH-HL \t-\t 1\n",
      "WRB-HL \t-\t 36\n",
      "WDT+BEZ-HL \t-\t 1\n",
      "NP$-HL \t-\t 8\n",
      "WDT-HL \t-\t 30\n",
      "DOZ-HL \t-\t 16\n",
      "RB-NC \t-\t 26\n",
      "---HL \t-\t 26\n",
      "PP$-HL \t-\t 10\n",
      ":-TL \t-\t 22\n",
      "RN \t-\t 9\n",
      "VBZ-TL \t-\t 17\n",
      "FW-PPL \t-\t 9\n",
      "WPO-TL \t-\t 4\n",
      "NRS \t-\t 16\n",
      "BEZ-TL \t-\t 8\n",
      "PPO-TL \t-\t 13\n",
      "ABN-TL \t-\t 7\n",
      "FW-NP \t-\t 7\n",
      "PPL-HL \t-\t 1\n",
      "PPSS-TL \t-\t 9\n",
      "IN-NC \t-\t 41\n",
      "FW-VBZ \t-\t 4\n",
      "FW-PPO \t-\t 4\n",
      "DO-TL \t-\t 5\n",
      "QL-HL \t-\t 4\n",
      "DT-TL \t-\t 9\n",
      "FW-NNS-TL \t-\t 36\n",
      "FW-VBD-TL \t-\t 1\n",
      "WRB+BEZ-TL \t-\t 3\n",
      "WRB-TL \t-\t 9\n",
      "WQL-TL \t-\t 5\n",
      "FW-CC-TL \t-\t 14\n",
      "FW-AT+NP-TL \t-\t 2\n",
      "FW-OD-TL \t-\t 4\n",
      "FW-AT+NN-TL \t-\t 13\n",
      "NIL \t-\t 157\n",
      "FW-IN+AT-T \t-\t 3\n",
      "PPSS+BER-TL \t-\t 1\n",
      "WPS-TL \t-\t 12\n",
      "HVZ-TL \t-\t 4\n",
      "BEN-TL \t-\t 2\n",
      "RP-TL \t-\t 4\n",
      "CD-TL-HL \t-\t 17\n",
      "BE-TL \t-\t 1\n",
      "IN-TL-HL \t-\t 6\n",
      "HV* \t-\t 42\n",
      "CS-TL \t-\t 2\n",
      "PPSS+HV-TL \t-\t 1\n",
      "DOD*-TL \t-\t 1\n",
      "PPS-TL \t-\t 6\n",
      "FW-RB-TL \t-\t 3\n",
      "PN-TL \t-\t 5\n",
      "WDT+BEZ-TL \t-\t 1\n",
      "NR-TL-HL \t-\t 5\n",
      "CC-TL-HL \t-\t 2\n",
      "AT-TL-HL \t-\t 5\n",
      "FW-JJ-NC \t-\t 2\n",
      "FW-JJR \t-\t 1\n",
      "FW-NN$ \t-\t 9\n",
      "FW-VBD \t-\t 2\n",
      ",-TL \t-\t 4\n",
      "HV-HL \t-\t 3\n",
      "VB+IN \t-\t 3\n",
      "WDT+BER+PP \t-\t 1\n",
      "VBG+TO \t-\t 17\n",
      "NN+HVZ \t-\t 5\n",
      "VBN+TO \t-\t 5\n",
      "FW-NN$-TL \t-\t 4\n",
      "FW-*-TL \t-\t 2\n",
      "NN+BEZ \t-\t 34\n",
      "FW-UH-NC \t-\t 1\n",
      "DO*-HL \t-\t 3\n",
      "PPS-HL \t-\t 19\n",
      "FW-PP$ \t-\t 3\n",
      "RB+BEZ-HL \t-\t 1\n",
      "JJS-HL \t-\t 1\n",
      "DTS-HL \t-\t 2\n",
      "NNS-NC \t-\t 26\n",
      "PPSS-NC \t-\t 31\n",
      "PPS-NC \t-\t 9\n",
      "PN-NC \t-\t 2\n",
      "FW-RB \t-\t 32\n",
      "FW-VBN \t-\t 12\n",
      "PPL-TL \t-\t 1\n",
      "WDT+HVZ \t-\t 2\n",
      "WRB+BEZ \t-\t 11\n",
      "WPS+MD \t-\t 8\n",
      "NP-NC \t-\t 15\n",
      "FW-IN+NP-TL \t-\t 2\n",
      "VBN-NC \t-\t 9\n",
      "AT-NC \t-\t 35\n",
      "FW-NN-NC \t-\t 6\n",
      "NN+MD \t-\t 2\n",
      "JJR+CS \t-\t 1\n",
      "FW-NNS-NC \t-\t 2\n",
      "PPS+HVD \t-\t 83\n",
      "VB+RP \t-\t 2\n",
      "NRS-TL \t-\t 1\n",
      "FW-BEZ \t-\t 4\n",
      "FW-BER \t-\t 3\n",
      "FW-VBG \t-\t 7\n",
      "FW-NR-TL \t-\t 3\n",
      "FW-PPSS \t-\t 6\n",
      "FW-HV \t-\t 1\n",
      "FW-NP-TL \t-\t 4\n",
      "VB-NC \t-\t 41\n",
      "DOZ*-TL \t-\t 1\n",
      "FW-NPS-TL \t-\t 1\n",
      "FW-PP$-TL \t-\t 2\n",
      "FW-CS \t-\t 3\n",
      "*-TL \t-\t 1\n",
      "HV-TL \t-\t 3\n",
      "FW-PN \t-\t 1\n",
      "FW-PPO+IN \t-\t 3\n",
      "DT-NC \t-\t 7\n",
      "JJT-TL \t-\t 4\n",
      "DTI-TL \t-\t 2\n",
      ".-TL \t-\t 2\n",
      "EX+MD \t-\t 4\n",
      "FW-UH \t-\t 8\n",
      "FW-NPS \t-\t 2\n",
      "FW-CD-TL \t-\t 2\n",
      "FW-BE \t-\t 1\n",
      "FW-PPS \t-\t 1\n",
      "FW-NR \t-\t 1\n",
      "FW-TO+VB \t-\t 1\n",
      "FW-IN+AT \t-\t 4\n",
      "JJ$-TL \t-\t 1\n",
      "FW-VB-TL \t-\t 1\n",
      "FW-RB+CC \t-\t 1\n",
      "FW-WPO \t-\t 1\n",
      "FW-NN-TL-NC \t-\t 1\n",
      "FW-WPS \t-\t 1\n",
      "FW-PPL+VBZ \t-\t 2\n",
      "DOZ-TL \t-\t 2\n",
      "VBN-TL-NC \t-\t 3\n",
      "NNS-TL-NC \t-\t 3\n",
      "NN-TL-NC \t-\t 3\n",
      "FW-DTS \t-\t 1\n",
      "NNS$-TL-HL \t-\t 1\n",
      "FW-VBG-TL \t-\t 1\n",
      "EX-HL \t-\t 1\n",
      "PPO-NC \t-\t 9\n",
      "PPSS+BER-N \t-\t 1\n",
      "WRB-NC \t-\t 7\n",
      "BER-NC \t-\t 5\n",
      "TO-NC \t-\t 13\n",
      "NR-NC \t-\t 4\n",
      "UH-NC \t-\t 5\n",
      "HV-NC \t-\t 11\n",
      ",-NC \t-\t 5\n",
      "WDT+BEZ-NC \t-\t 2\n",
      ".-NC \t-\t 16\n",
      "NP+HVZ-NC \t-\t 1\n",
      "HVZ-NC \t-\t 2\n",
      "CD-NC \t-\t 5\n",
      "QL-NC \t-\t 2\n",
      "WPS+BEZ-NC \t-\t 2\n",
      "PP$-NC \t-\t 13\n",
      "DT+BEZ-NC \t-\t 1\n",
      "RB+BEZ-NC \t-\t 1\n",
      "WDT-NC \t-\t 7\n",
      "BEDZ-NC \t-\t 8\n",
      "VBD-NC \t-\t 11\n",
      "RP-NC \t-\t 5\n",
      "VBZ-NC \t-\t 7\n",
      "VBG-NC \t-\t 16\n",
      "PPSS+MD-NC \t-\t 2\n",
      "*-NC \t-\t 1\n",
      "EX-NC \t-\t 1\n",
      "BER*-NC \t-\t 1\n",
      "AP-NC \t-\t 2\n",
      "DO-NC \t-\t 2\n",
      "BED-NC \t-\t 3\n",
      "CC-NC \t-\t 5\n",
      "PPS+BEZ-NC \t-\t 3\n",
      "MD-NC \t-\t 2\n",
      "PPSS+BER-NC \t-\t 1\n",
      "CS-NC \t-\t 5\n",
      "NNS$-NC \t-\t 2\n",
      "PPL-NC \t-\t 2\n",
      "RBR-NC \t-\t 1\n",
      "BEZ-NC \t-\t 5\n",
      "OD-NC \t-\t 1\n",
      "NP+BEZ-NC \t-\t 3\n",
      "ABN-NC \t-\t 1\n",
      "WPS-NC \t-\t 3\n",
      "JJT-NC \t-\t 1\n",
      "DOD-NC \t-\t 1\n",
      "WPO-NC \t-\t 1\n",
      "BEM-NC \t-\t 2\n",
      "NPS-NC \t-\t 2\n",
      "NN+NN-NC \t-\t 1\n",
      "JJ+JJ-NC \t-\t 2\n",
      "AP+AP-NC \t-\t 1\n",
      "VB+JJ-NC \t-\t 1\n",
      "VB+VB-NC \t-\t 1\n",
      "WPS-HL \t-\t 2\n",
      "FW-QL \t-\t 1\n",
      "JJ-TL-NC \t-\t 1\n",
      "FW-JJT \t-\t 1\n",
      "WPS+BEZ-TL \t-\t 1\n",
      "HVG-HL \t-\t 1\n",
      "FW-DT+BEZ \t-\t 2\n",
      "EX+HVD \t-\t 3\n",
      "WPS+HVZ \t-\t 2\n",
      "PN+MD \t-\t 3\n",
      "VB+TO \t-\t 4\n",
      "DT+MD \t-\t 3\n",
      "HV+TO \t-\t 3\n",
      "MD+TO \t-\t 2\n",
      "MD+PPSS \t-\t 1\n",
      "NR+MD \t-\t 1\n",
      "NN+IN \t-\t 1\n",
      "RP+IN \t-\t 4\n",
      "BEM* \t-\t 9\n",
      "PN+BEZ \t-\t 7\n",
      "WPS+HVD \t-\t 6\n",
      "NN+HVD-TL \t-\t 1\n",
      "WDT+DOD \t-\t 1\n",
      "NN+BEZ-TL \t-\t 2\n",
      "WRB+DO \t-\t 1\n",
      "WRB+IN \t-\t 1\n",
      "NP+HVZ \t-\t 6\n",
      "WRB+DOD \t-\t 6\n",
      "WRB+MD \t-\t 1\n",
      "EX+HVZ \t-\t 2\n",
      "NN+HVZ-TL \t-\t 1\n",
      "PPSS+VB \t-\t 2\n",
      "WRB+BER \t-\t 1\n",
      "NNS+MD \t-\t 2\n",
      "PPSS+BEZ \t-\t 1\n",
      "PPSS+BEZ* \t-\t 1\n",
      "RBR+CS \t-\t 1\n",
      "NP+MD \t-\t 2\n",
      "TO+VB \t-\t 2\n",
      "IN+PPO \t-\t 1\n",
      "IN+IN \t-\t 1\n",
      "DO+PPSS \t-\t 1\n",
      "VB+AT \t-\t 2\n",
      "WRB+DOZ \t-\t 1\n",
      "DTS+BEZ \t-\t 2\n",
      "WDT+DO+PPS \t-\t 1\n",
      "RB+CS \t-\t 3\n",
      "WRB+DOD* \t-\t 1\n",
      "WDT+BER \t-\t 1\n",
      "FW-OD-NC \t-\t 1\n",
      "FW-PPSS+HV \t-\t 1\n",
      "PN+HVD \t-\t 1\n",
      "FW-UH-TL \t-\t 1\n"
     ]
    }
   ],
   "source": [
    "WordTypeCount = {}\n",
    "WordTypes = brown.tagged_words()\n",
    "\n",
    "for word_type in WordTypes:\n",
    "    if word_type[1] in WordTypeCount:\n",
    "        WordTypeCount[word_type[1]] += 1\n",
    "    else:\n",
    "        WordTypeCount[word_type[1]] = 1\n",
    "\n",
    "print(len(set(brown.raw())))\n",
    "for word_type in WordTypeCount:\n",
    "    print(word_type,\"\\t-\\t\",WordTypeCount[word_type])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Find the size of the category “government”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of category government in Brown Corpus:  70117\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of category government in Brown Corpus: \",len(brown.words(categories='government')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• List the most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 62713)\n",
      "(',', 58334)\n",
      "('.', 49346)\n",
      "('of', 36080)\n",
      "('and', 27915)\n",
      "('to', 25732)\n",
      "('a', 21881)\n",
      "('in', 19536)\n",
      "('that', 10237)\n",
      "('is', 10011)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 62713),\n",
       " (',', 58334),\n",
       " ('.', 49346),\n",
       " ('of', 36080),\n",
       " ('and', 27915),\n",
       " ('to', 25732),\n",
       " ('a', 21881),\n",
       " ('in', 19536),\n",
       " ('that', 10237),\n",
       " ('is', 10011)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordFrequency = {}\n",
    "tokens = brown.words()\n",
    "for word in tokens:\n",
    "    if word in WordFrequency:\n",
    "        WordFrequency[word] += 1\n",
    "    else:\n",
    "        WordFrequency[word] = 1\n",
    "\n",
    "SortedWords = sorted(WordFrequency.items(),key = lambda x:x[1], reverse=True)\n",
    "for i in range(10):\n",
    "    print(SortedWords[i])\n",
    "\n",
    "FreqDistWords = FreqDist(tokens)\n",
    "FreqDistWords.most_common(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Count the number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences:  55635\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\DARSHINI\\VIT\\SEMESTER_6 Winter(22-23)\\Natural Language Processing\\Assignments\\01_BrownCorpus.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/DARSHINI/VIT/SEMESTER_6%20Winter%2822-23%29/Natural%20Language%20Processing/Assignments/01_BrownCorpus.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         NumOfSentence \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/DARSHINI/VIT/SEMESTER_6%20Winter%2822-23%29/Natural%20Language%20Processing/Assignments/01_BrownCorpus.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNumber of Sentences: \u001b[39m\u001b[39m\"\u001b[39m,NumOfSentence)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/DARSHINI/VIT/SEMESTER_6%20Winter%2822-23%29/Natural%20Language%20Processing/Assignments/01_BrownCorpus.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mlen\u001b[39m(sent_tokenize(brown\u001b[39m.\u001b[39;49mraw()))\n",
      "File \u001b[1;32mc:\\Users\\darsh\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtokenizers/punkt/\u001b[39m\u001b[39m{\u001b[39;00mlanguage\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\darsh\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1276\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1273\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m \u001b[39m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[1;32mc:\\Users\\darsh\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\Users\\darsh\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\Users\\darsh\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1322\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[39mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1321\u001b[0m     slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1322\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m slices:\n\u001b[0;32m   1323\u001b[0m     \u001b[39myield\u001b[39;00m (sentence\u001b[39m.\u001b[39mstart, sentence\u001b[39m.\u001b[39mstop)\n",
      "File \u001b[1;32mc:\\Users\\darsh\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1421\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[39mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[39mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[39m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m realign \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1421\u001b[0m \u001b[39mfor\u001b[39;00m sentence1, sentence2 \u001b[39min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1422\u001b[0m     sentence1 \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(sentence1\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m realign, sentence1\u001b[39m.\u001b[39mstop)\n\u001b[0;32m   1423\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32mc:\\Users\\darsh\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:318\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    316\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(iterator)\n\u001b[0;32m    317\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     prev \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[0;32m    319\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\darsh\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_slices_from_text\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m   1394\u001b[0m     last_break \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1395\u001b[0m     \u001b[39mfor\u001b[39;00m match, context \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_match_potential_end_contexts(text):\n\u001b[0;32m   1396\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1397\u001b[0m             \u001b[39myield\u001b[39;00m \u001b[39mslice\u001b[39m(last_break, match\u001b[39m.\u001b[39mend())\n",
      "File \u001b[1;32mc:\\Users\\darsh\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1381\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1379\u001b[0m \u001b[39m# Find the word before the current match\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m split \u001b[39m=\u001b[39m text[: match\u001b[39m.\u001b[39mstart()]\u001b[39m.\u001b[39mrsplit(maxsplit\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m-> 1381\u001b[0m before_start \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(split[\u001b[39m0\u001b[39m]) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39;49m(split) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m   1382\u001b[0m before_words[match] \u001b[39m=\u001b[39m split[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m split \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m matches\u001b[39m.\u001b[39mappend(match)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NumOfSentence = 0\n",
    "for i in brown.words():\n",
    "    if(i=='.' or i=='!' or i=='?'):\n",
    "        NumOfSentence +=1 \n",
    "\n",
    "print(\"Number of Sentences: \",NumOfSentence)\n",
    "\n",
    "len(sent_tokenize(brown.raw()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b364cb0efb2e8f1848db57652adf6c7915ae4a4f54bc1cd1c0717e1cad4299e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
